+++
title = "LML - linear regression"
date = 2024-05-01 23:43:25
+++

LML 是 Learn Machine Learning 的缩写，我会用文字和代码，从一个程序员的角度，记录下自己这段时间学习机器学习的过程。

这是第一篇：线性回归（与 softmax 回归）

<!-- more -->

- - -

## 理论篇

先从程序员的角度，从结论出发，一步步带着问题进行答案寻找我们要学习的内容，线性回归和 softmax 回归。

### 什么是线性回归

1. 什么是回归

回归通常用来表示输入和输出的关系，比方说 y 是输出，x 是输入，他们之间存在某种联系，那么我们在建模这个问题的时候，有一类方法就叫做回归（regression）

而回归的目标通常是预测某个指标，比方说通过输入一个人的电脑使用时间长短和他头发的长短，预测出这个人是不是程序员，等等，而预测，往往是可以通过分类来实现，比方说我们知道的职业有，程序员，厨师，司机等等，那么我们的任务其实就是通过输入来给它进行分类。

2. 什么是线性回归

作为理工科学生，线性可以直观地从直线函数 y = ax + b 开始理解，在机器学习领域，我们把上面的公式推广到高维，意味着，y 可以表示为 x 的加权和加上一个（或一系列）常数，这种模型就叫做线性回归。

既然涉及到权重，在机器学习里面，通常用 w 表示，在这个模型中， w 和 x 是维度一样的数组，他们的加权和也就是矩阵乘法的点积。因此也可以用矩阵乘法来表示

那么模型的数学表达就是 y = x * w.T + b (.T 表示转置，这里用转置才能与 x 点积)

到这里，y, x, w, b 就都可以是高维度的了，可以表达很多具体问题

3. 更进一步

上面的模型 y = x * w.T + b ，我们也成为线性变换或是仿射变换。

从直线开始理解，直线函数  y = ax + b 里面，通过改变 a, b 的值，直线被平移、旋转，但它最后一定是一条直线，不可能是曲线或是转弯。

提升一个维度，多了一个 z 变量, y = ax + bz + c，表达一定总是一个平面，不可能有弯曲（写成 y = w1 * x1 + w2 * x2 + b 是等同的）。

再继续提升维度，我们没办法画出更高维度的图像，但是这个性质一直会保持，我们把这个高维空间的表达叫做超平面。


### 线性回归

TODO

https://github.com/chux0519/lml/blob/master/linear

### softmax 回归

TODO

https://github.com/chux0519/lml/blob/master/softmax/